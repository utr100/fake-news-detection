{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "fake_news_detection.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPNZRAjXfgUi53JOMGicZ/w",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/utr100/fake-news-detection/blob/main/fake_news_detection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O3vabsK8EWY2"
      },
      "source": [
        "# Fake News Detection Using Gated Recurrent Units"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tcfa1LjpAXJG"
      },
      "source": [
        "The objective of this notebook is to demonstrate how to use a machine learning program to identify when an article might be fake news. The dataset for this challenge can be found on [Kaggle](https://www.kaggle.com/c/fake-news/data). \n",
        "\n",
        "To achieve this, we will use a type of neural network called Gated Recurrent Units (GRU's) which are suitable for sequence data such as text and time series. GRU's can be considered as an improvement to simple Recurrent Neural Networks (RNN's), and they alleviate some of the issues with RNN's to some extent (for example the extremely short-term memory of RNN's)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yBlBMGmGFi1B"
      },
      "source": [
        "## Importing Libraries"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j6Fj26YaBpc0"
      },
      "source": [
        "We will be using [Tensorflow](https://www.tensorflow.org/) to train the GRU model for fake news detection. Along with Tensorflow, we will use useful functions from some other libraries such as [Pandas](https://pandas.pydata.org/) and [Scikit-Learn](https://scikit-learn.org/) to help prepare our data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FXpxrdQMFfRk"
      },
      "source": [
        "# Python ≥3.5 is required\n",
        "import sys\n",
        "assert sys.version_info >= (3, 5)\n",
        "\n",
        "import pandas as pd\n",
        "import shutil\n",
        "\n",
        "# TensorFlow ≥2.0 is required\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "assert tf.__version__ >= \"2.0\"\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "# Scikit-Learn ≥0.20 is required\n",
        "import sklearn\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "assert sklearn.__version__ >= \"0.20\""
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YRhMfWI4Ej6z"
      },
      "source": [
        "## Getting the Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8fR1u7o7EnGx"
      },
      "source": [
        "Note: Since the training data is larger than 25MB, we cannot upload it to a Github repository. So, we will use the Kaggle API to download it directly from the source.\n",
        "\n",
        "Instructions to set up and use Kaggle API can be found [here](https://github.com/Kaggle/kaggle-api).\n",
        "\n",
        "If you would not like to set up the Kaggle API you can also download the data to your system, unzip it, and upload it to colab.\n",
        "\n",
        "**Note: Skip this section if you are uploading the data yourself. If you are uploading your own data, make sure that you upload both the train.csv and test.csv files to the /content directory.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "maWxrYaaDih-"
      },
      "source": [
        "# Upload the kaggle.json file into the /content directory and then run the following code\n",
        "! mkdir ~/.kaggle\n",
        "! cp kaggle.json ~/.kaggle/\n",
        "! chmod 600 ~/.kaggle/kaggle.json"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5kxMCfgD0NKw",
        "outputId": "a3392934-1e89-43ba-9eef-e4d087576d55"
      },
      "source": [
        "# Download the data files\n",
        "! kaggle competitions download -c fake-news"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Warning: Looks like you're using an outdated API Version, please consider updating (server 1.5.12 / client 1.5.4)\n",
            "Downloading submit.csv to /content\n",
            "  0% 0.00/40.6k [00:00<?, ?B/s]\n",
            "100% 40.6k/40.6k [00:00<00:00, 16.1MB/s]\n",
            "Downloading test.csv.zip to /content\n",
            " 53% 5.00M/9.42M [00:00<00:00, 49.5MB/s]\n",
            "100% 9.42M/9.42M [00:00<00:00, 60.1MB/s]\n",
            "Downloading train.csv.zip to /content\n",
            " 70% 26.0M/37.0M [00:00<00:00, 68.2MB/s]\n",
            "100% 37.0M/37.0M [00:00<00:00, 106MB/s] \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yRUoMzGlFpJe"
      },
      "source": [
        "# Unzip the train and test zip files\n",
        "shutil.unpack_archive('train.csv.zip')\n",
        "shutil.unpack_archive('test.csv.zip')"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aS6TqEXMNHMv"
      },
      "source": [
        "## Reading the Data and Getting Info"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KC-IkYipM810"
      },
      "source": [
        "full_train_df = pd.read_csv('train.csv')"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wTf4VrydDAbM"
      },
      "source": [
        "There are some null values in the dataset. We will drop the null values in our analysis."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F_LL2QgsNC1O",
        "outputId": "aae3d390-b238-4868-846e-bf02c76eb2df"
      },
      "source": [
        "full_train_df.info()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 20800 entries, 0 to 20799\n",
            "Data columns (total 5 columns):\n",
            " #   Column  Non-Null Count  Dtype \n",
            "---  ------  --------------  ----- \n",
            " 0   id      20800 non-null  int64 \n",
            " 1   title   20242 non-null  object\n",
            " 2   author  18843 non-null  object\n",
            " 3   text    20761 non-null  object\n",
            " 4   label   20800 non-null  int64 \n",
            "dtypes: int64(2), object(3)\n",
            "memory usage: 812.6+ KB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fK4NtlsaDUVm"
      },
      "source": [
        "There are three columns which may be useful in determining whether a news article is fake or not - title, author, and text. In our analysis, we will use only the text column to classify the articles as this column contains the most important information."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "eNqKU3QmNSB8",
        "outputId": "d047b9f3-4923-474f-b1ce-a8d0c82339b1"
      },
      "source": [
        "full_train_df.head()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>title</th>\n",
              "      <th>author</th>\n",
              "      <th>text</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>House Dem Aide: We Didn’t Even See Comey’s Let...</td>\n",
              "      <td>Darrell Lucus</td>\n",
              "      <td>House Dem Aide: We Didn’t Even See Comey’s Let...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>FLYNN: Hillary Clinton, Big Woman on Campus - ...</td>\n",
              "      <td>Daniel J. Flynn</td>\n",
              "      <td>Ever get the feeling your life circles the rou...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>Why the Truth Might Get You Fired</td>\n",
              "      <td>Consortiumnews.com</td>\n",
              "      <td>Why the Truth Might Get You Fired October 29, ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>15 Civilians Killed In Single US Airstrike Hav...</td>\n",
              "      <td>Jessica Purkiss</td>\n",
              "      <td>Videos 15 Civilians Killed In Single US Airstr...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>Iranian woman jailed for fictional unpublished...</td>\n",
              "      <td>Howard Portnoy</td>\n",
              "      <td>Print \\nAn Iranian woman has been sentenced to...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   id  ... label\n",
              "0   0  ...     1\n",
              "1   1  ...     0\n",
              "2   2  ...     1\n",
              "3   3  ...     1\n",
              "4   4  ...     1\n",
              "\n",
              "[5 rows x 5 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c4kQMcr_g3-O"
      },
      "source": [
        "This is a very well-balanced dataset with roughly equal instances of positive and negative classes. This balance is generally favorable for Machine Learning algorithms."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XCUJa3fcgw0r",
        "outputId": "fe0fe267-1637-4749-d782-1a5bc9b146bb"
      },
      "source": [
        "full_train_df['label'].value_counts()"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1    10413\n",
              "0    10387\n",
              "Name: label, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CCp2b_vpD4aX"
      },
      "source": [
        "## Preparing the Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VV7tJafZEdCk"
      },
      "source": [
        "We will use the following steps to prepare the data:\n",
        "1. **Removing null values**: Since we are using the text column in determining whether a news article may be fake, we will have to remove the rows which have null values in the text column.\n",
        "2. **Separating out the target column**\n",
        "3. **Splitting the dataset into train, validation, and test**\n",
        "\n",
        "There are some other preprocessing steps such as **removing stop words** and **stemming/lemmatization** that we could have applied, but in this demonstration, we are going to proceed without these steps.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WBzad-COUcE5"
      },
      "source": [
        "# Removing articles with null values in text column\n",
        "full_train_df = full_train_df[full_train_df['text'].notnull()].reset_index(drop=True).copy()"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "km30w3mJTXG6"
      },
      "source": [
        "# Seperating out the target column\n",
        "full_X_df = full_train_df.drop([\"label\"], axis=1)\n",
        "full_y_df = full_train_df[\"label\"]"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lHRRStb7UHOI"
      },
      "source": [
        "# Splitting the dataset into train, validation and test sets\n",
        "\n",
        "# 10% data for validation \n",
        "X_train_df, X_validation_df, y_train, y_validation = train_test_split(\n",
        "    full_X_df, full_y_df, test_size=0.10, random_state=42)\n",
        "\n",
        "# 10% data for test \n",
        "X_train_df, X_test_df, y_train, y_test = train_test_split(\n",
        "    X_train_df, y_train, test_size=0.10, random_state=42)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ex62KEzKEE4"
      },
      "source": [
        "## Tokenizing the Text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MJoYqyG9RN-d"
      },
      "source": [
        "Machine Learning algorithms cannot handle text data directly. So, text data has to be encoded into a numerical format before it can be fed into Machine Learning models. To do this for our neural network, we will use the `keras.preprocessing.text.Tokenizer` class, which can be fitted on text data and then it can map each token (a token is typically a word, but it can also be set as a character or a subword) to an integer. For example, if the word **news** has been mapped to the integer **11**, then all instances of the word **news** in the data will get replaced by **11**.\n",
        "\n",
        "After fitting the tokenizer, it can be noticed that the vocabulary size is quite large (208,474 words). We would not need all the words in order to make accurate predictions, so we will only keep the 15,000 most frequent words. All the other words will be considered as unknown and will be mapped to the value of the \\<unk\\> token. (see code below)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pXPOtuGWxbxm"
      },
      "source": [
        "# converting text column to numpy array\n",
        "train_text = X_train_df['text'].apply(lambda x: str(x)).to_numpy()\n",
        "\n",
        "# Initializing and fitting the tokenizer\n",
        "tokenizer = keras.preprocessing.text.Tokenizer(char_level=False, oov_token='<unk>')\n",
        "tokenizer.fit_on_texts(train_text)\n",
        "\n",
        "# creating a token for padding - padding is used to make all training sentences of \n",
        "# the same size by adding the <pad> token to the beginning or end of shorter sentencs\n",
        "tokenizer.word_index['<pad>'] = 0\n",
        "tokenizer.index_word[0] = '<pad>'"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6xxpK_ThbWHd",
        "outputId": "b11a626e-cf91-4f73-9981-14c1518610b2"
      },
      "source": [
        "# The vocabulary size is quite large\n",
        "len(tokenizer.word_counts)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "208355"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B7cj2nzha_gH"
      },
      "source": [
        "# Restricting the vocabulary size\n",
        "tokenizer.num_words = 15000"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aoFhvbPYLpDT"
      },
      "source": [
        "## Encoding and Padding the Train, Validation and Test Texts"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6KOx7_68WRTU"
      },
      "source": [
        "Now we will use the tokenizer that we have fitted to encode the train, validation, and test datasets. Then we will pad the news articles to make them all of the same length."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ShowZKzyUyGZ"
      },
      "source": [
        "# converting text column to numpy array\n",
        "validation_text = X_validation_df['text'].apply(lambda x: str(x)).to_numpy()\n",
        "test_text = X_test_df['text'].apply(lambda x: str(x)).to_numpy()"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6H7pOmfdx7bH"
      },
      "source": [
        "# tokenizing the texts\n",
        "X_train = tokenizer.texts_to_sequences(train_text)\n",
        "X_validation = tokenizer.texts_to_sequences(validation_text)\n",
        "X_test = tokenizer.texts_to_sequences(test_text)\n",
        "\n",
        "# padding the texts\n",
        "X_train = tf.keras.preprocessing.sequence.pad_sequences(X_train, padding='post')\n",
        "X_validation = tf.keras.preprocessing.sequence.pad_sequences(X_validation, padding='post')\n",
        "X_test = tf.keras.preprocessing.sequence.pad_sequences(X_test, padding='post')"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LdXWOCM8MYwQ"
      },
      "source": [
        "## Final Preparation for Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QhJPUnNyWvDJ"
      },
      "source": [
        "As part of the final data preparation, we will perform the following operations on the data:\n",
        "\n",
        "1. We will truncate each article to 200 words, since the first 200 words should be enough to determine if the article constitutes fake news.\n",
        "2. We will convert the labels to a NumPy array.\n",
        "3. We will convert the articles and the labels from NumPy arrays to Tensorflow datasets. This format can then be fed into the neural network.\n",
        "4. We will batch and prefetch the datasets."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xzHA2jMByJCM"
      },
      "source": [
        "# Keep the first 200 words of each article\n",
        "X_train = [text[:200] for text in X_train]\n",
        "X_validation = [text[:200] for text in X_validation]\n",
        "X_test = [text[:200] for text in X_test]"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lb-oxipEyilE"
      },
      "source": [
        "# converting the labels to numpy array\n",
        "y_train = y_train.to_numpy().flatten()\n",
        "y_validation = y_validation.to_numpy().flatten()\n",
        "y_test = y_test.to_numpy().flatten()"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "om1tKct1zXYs"
      },
      "source": [
        "# converting the data from numpy arrays to tensorflow datasets\n",
        "tfds_train = tf.data.Dataset.from_tensor_slices((X_train, y_train))\n",
        "tfds_validation = tf.data.Dataset.from_tensor_slices((X_validation, y_validation))\n",
        "tfds_test = tf.data.Dataset.from_tensor_slices((X_test, y_test))"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "88hrzRdp3C73"
      },
      "source": [
        "# batching and prefetching\n",
        "tfds_train = tfds_train.batch(16).prefetch(1)\n",
        "tfds_validation = tfds_validation.batch(16).prefetch(1)\n",
        "tfds_test = tfds_test.batch(16).prefetch(1)"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ycQViYouMsC2"
      },
      "source": [
        "## Creating and Training the Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EtVwuafzXnBh"
      },
      "source": [
        "Finally, it's time to create and train the model. Below are steps to achieve this:\n",
        "\n",
        "1. We create a sequential model which consists of an embedding layer as the input layer, followed by 2 GRU layers and finally a dense layer with 1 neuron and sigmoid activation function. (The last layer is the conventional architecture used for binary classification).\n",
        "2. We then compile our model with the binary cross entropy as the loss (typical for binary classification tasks), adam optimizer (a very fast optimizer) and accuracy as the metric.\n",
        "3. Finally we fit our model to the data and train it for 3 epochs, using the validation dataset created earlier for validation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1BzkiCyk3XDB",
        "outputId": "69117953-991a-4bf0-c956-e213b85d0801"
      },
      "source": [
        "vocab_size = tokenizer.num_words\n",
        "embed_size = 128\n",
        "model = keras.models.Sequential([\n",
        "    keras.layers.Embedding(vocab_size, embed_size,\n",
        "                           mask_zero=True,\n",
        "                           input_shape=[None]),\n",
        "    keras.layers.GRU(128, return_sequences=True),\n",
        "    keras.layers.GRU(128),\n",
        "    keras.layers.Dense(1, activation=\"sigmoid\")\n",
        "])\n",
        "model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
        "history = model.fit(tfds_train, epochs=3, validation_data=tfds_validation)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/3\n",
            "1051/1051 [==============================] - 228s 204ms/step - loss: 0.3074 - accuracy: 0.8501 - val_loss: 0.1340 - val_accuracy: 0.9509\n",
            "Epoch 2/3\n",
            "1051/1051 [==============================] - 210s 200ms/step - loss: 0.0517 - accuracy: 0.9824 - val_loss: 0.1284 - val_accuracy: 0.9687\n",
            "Epoch 3/3\n",
            "1051/1051 [==============================] - 210s 200ms/step - loss: 0.0236 - accuracy: 0.9933 - val_loss: 0.1475 - val_accuracy: 0.9658\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1b8vVS8lj3Ou"
      },
      "source": [
        "At the end of 3 epochs, the accuracy achieved on the training data is 99.33%, whereas the accuracy achieved on the validation data is 96.58%. Since the training accuracy is greater than the validation accuracy, there is some measure of overfitting involved during training, which can be addressed by applying regularization measures such as adding dropout layers in the network and training the model again."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jZDX2nuSRARC"
      },
      "source": [
        "## Evaluation on Test Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1clsCoCfo1A-"
      },
      "source": [
        "We now evaluate our model on the 10% of test data that we had set aside in the beginning. It can be observed in the below report that the precision, recall and f1-scores for both the classes are reasonable high (97%), which indicates that the classifier is performing well on the test data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vbpFfxMOcgov",
        "outputId": "8884bc98-7d88-4c10-a396-e5931f580d8c"
      },
      "source": [
        "# make predictions using model\n",
        "predictions = model.predict(tfds_test)\n",
        "\n",
        "# function to convert the scores to binary classes (0 or 1)\n",
        "def get_class(score):\n",
        "  return 1 if score > 0.5 else 0\n",
        "\n",
        "get_class_v = np.vectorize(get_class)\n",
        "\n",
        "# get the binary class values\n",
        "y_pred = get_class_v(predictions)\n",
        "\n",
        "# get the classification report which contains precison and recall values for each class\n",
        "target_names = ['reliable', 'unreliable']\n",
        "print(classification_report(y_test, y_pred, target_names=target_names))"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "    reliable       0.96      0.98      0.97       927\n",
            "  unreliable       0.98      0.96      0.97       942\n",
            "\n",
            "    accuracy                           0.97      1869\n",
            "   macro avg       0.97      0.97      0.97      1869\n",
            "weighted avg       0.97      0.97      0.97      1869\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nueTPWE7iRId"
      },
      "source": [
        "## Creating Submission File"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NMdLCXQQqBH5"
      },
      "source": [
        "We now create the submission file using the data in the unlabeled \"test.csv\" file."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O7vSnDuFiQRP"
      },
      "source": [
        "test_df_final = pd.read_csv('test.csv')"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PTecaZjviXV5",
        "outputId": "c7ce93ed-460e-4bec-ba94-9600d8ed7584"
      },
      "source": [
        "test_df_final.info()"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 5200 entries, 0 to 5199\n",
            "Data columns (total 4 columns):\n",
            " #   Column  Non-Null Count  Dtype \n",
            "---  ------  --------------  ----- \n",
            " 0   id      5200 non-null   int64 \n",
            " 1   title   5078 non-null   object\n",
            " 2   author  4697 non-null   object\n",
            " 3   text    5193 non-null   object\n",
            "dtypes: int64(1), object(3)\n",
            "memory usage: 162.6+ KB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h9DfhQRvidxQ"
      },
      "source": [
        "# Fill null values with empty string\n",
        "test_df_final.replace(np.NaN, '', inplace=True)\n",
        "\n",
        "# converting text column to numpy array\n",
        "test_text_final = test_df_final['text'].apply(lambda x: str(x)).to_numpy()\n",
        "\n",
        "# tokenizing and padding the texts\n",
        "X_test_final = tokenizer.texts_to_sequences(test_text_final)\n",
        "X_test_final = tf.keras.preprocessing.sequence.pad_sequences(X_test_final, padding='post')\n",
        "\n",
        "# Keep the first 200 words of each article\n",
        "X_test_final = [text[:200] for text in X_test_final]\n",
        "\n",
        "# converting the data from numpy arrays to tensorflow datasets, batching and prefetching\n",
        "tfds_test_final = tf.data.Dataset.from_tensor_slices((X_test_final))\n",
        "tfds_test_final = tfds_test_final.batch(16).prefetch(1)\n",
        "\n",
        "# get the predictions\n",
        "predictions_final = model.predict(tfds_test_final)\n",
        "\n",
        "# get the binary class values\n",
        "y_pred_final = get_class_v(predictions_final)"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mQzki_U43XCZ"
      },
      "source": [
        "# create and save final submission df\n",
        "test_df_final['label'] = y_pred_final\n",
        "test_df_final = test_df_final[['id', 'label']]\n",
        "test_df_final.to_csv('submit.csv', index=False)"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7IMDbxZhn5L9"
      },
      "source": [
        "## Testing on custom text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "75Q-0UBAyHys"
      },
      "source": [
        "In this section we can try out the model using our custom text."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "cellView": "form",
        "id": "vATSNS4Zz1Rx",
        "outputId": "7c16e331-2819-43c0-9f69-b2a276939b2e"
      },
      "source": [
        "#@title Fake News Detector\n",
        "\n",
        "new_text = \"Our house is burning. Literally. The Amazon rain forest - the lungs which produces 20% of our planet\\u2019s oxygen - is on fire. It is an international crisis. Members of the G7 Summit, let's discuss this emergency first order in two days!\" #@param {type:\"string\"}\n",
        "\n",
        "new_text_processed = [new_text]\n",
        "\n",
        "X_new = tokenizer.texts_to_sequences(new_text_processed)\n",
        "X_new = tf.keras.preprocessing.sequence.pad_sequences(X_new, padding='post')\n",
        "\n",
        "# Keep the first 200 words of each article\n",
        "X_new = [text[:200] for text in X_new]\n",
        "\n",
        "tfds_new = tf.data.Dataset.from_tensor_slices((X_new))\n",
        "tfds_new = tfds_new.batch(16).prefetch(1)\n",
        "\n",
        "score = model.predict(tfds_new)\n",
        "classification = \"unreliable\" if score > 0.5 else \"reliable\"\n",
        "\n",
        "print(f\"Score: {round(float(score), 4)}\")\n",
        "print(f\"The news is likely {classification}\")"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Score: 0.9995\n",
            "The news is likely unreliable\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rr6_EUsOpvYk"
      },
      "source": [
        "## Conclusion and Improvement Ideas"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RwDhUE9cp0MU"
      },
      "source": [
        "It was possible to train a fairly performant classifier with a minimum of data preprocessing and a relatively simple model, with 1 embedding layer, 2 GRU layers and a single neuron for producing the final classification score. There are a number of improvements that can be applied to this simple workflow in order to improve its performance. Listed below are some ideas for improvement:\n",
        "1.\tText preprocessing steps such as removing stopwords and stemming/lemmatization can be applied to the data before encoding it.\n",
        "2.\tA pre-trained embedding can be used in the embedding layer. These embeddings are usually trained on a much larger corpus of data and perform better that an embedding trained from scratch.\n",
        "3.\tThe embedding and GRU architecture can be replaced with superior Transformer based architectures which are the current state-of-the-art in Natural Language Processing tasks. One such example is BERT (Bidirectional Encoder Representation from Transformers) which is available in different sizes as a pre-trained model and can be fine tuned to any task, including classification. \n"
      ]
    }
  ]
}